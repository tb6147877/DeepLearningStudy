{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch : Build an AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network that learns to copy its input to its output. In autoencoder, encoder encodes the image into compressed representation, and the decoder decodes the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](denoising_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of AutoEncoder:\n",
    "\n",
    "- Segmentation\n",
    "- Denoising Image\n",
    "- Super Resolution Image\n",
    "- Image Compression\n",
    "- and many more .....\n",
    "\n",
    "In this Project, you will see the simplest implemention of autoencoder for image denoising task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T16:46:38.504101900Z",
     "start_time": "2024-06-01T16:46:36.796350100Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m \n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnotebook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm \n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mnist_dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.iloc[7] # selecting image at index 7 \n",
    "image = np.array(image).astype('float32')\n",
    "image = np.reshape(image,(28,28)) # reshaping image to 28 x 28 \n",
    "image = image / 255 # to scale image between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image,cmap = \"gray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_image = image + 0.2 * np.random.randn(*image.shape) # Adding random noise to image \n",
    "noisy_image = np.clip(noisy_image,0,1) #ranging between 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_image,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from helper import show_image, ToTensorForAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dataset.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_AE_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,csv_file,noise_factor = 0.2,transform = None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform \n",
    "        self.noise_factor = noise_factor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        img = self.data.iloc[idx]\n",
    "        img = np.array(img)\n",
    "        img = np.reshape(img,(28,28,1)) / 255\n",
    "        \n",
    "        noisy_img = img + self.noise_factor * np.random.randn(*img.shape)\n",
    "        noisy_img = np.clip(noisy_img,0.,1.)\n",
    "        \n",
    "        sample = (noisy_img,img)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Here to ToTensorForAE() is a function which will convert numpy,PIL image to torch tensor and also it will convert image with (height,width,channel) to (channel,height,width) as per the pytorch model input convention.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg, numpy array image with shape (28,28,1) will be first converted to image torch tensor with shape(28,28,1)\n",
    "and then the channel is shifted to 0th axis so image tensor will be with shape (1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST_AE_Dataset('mnist_dataset.csv',transform = ToTensorForAE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of trainset : {}\".format(len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_image,org_image = trainset[6]\n",
    "noisy_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(noisy_image,org_image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset,batch_size = 16,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "noisy_image, org_image = dataiter.next()\n",
    "\n",
    "print(\"Shape of loading one batch : {}\".format(noisy_image.shape))\n",
    "print(\"Shape of loading one batch : {}\".format(org_image.shape))\n",
    "\n",
    "print(\"Total no. of batches present in trainloader : {}\".format(len(trainloader))) #total no. of batches\n",
    "print(\"Total no. of examples present in trainloader : {}\".format(len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AutoEncoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder():\n",
    "    \n",
    "    enc = nn.Sequential(\n",
    "        \n",
    "        nn.Conv2d(in_channels = 1, out_channels = 16,kernel_size = (3,3),padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = (2,2)), \n",
    "        nn.Conv2d(in_channels = 16, out_channels = 32,kernel_size = (3,3),padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = (2,2))\n",
    "    )\n",
    "    \n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder():\n",
    "    \n",
    "    dec = nn.Sequential(\n",
    "        \n",
    "        nn.ConvTranspose2d(in_channels = 32,out_channels = 16,kernel_size = (2,2),stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(in_channels = 16,out_channels = 1,kernel_size = (2,2),stride = 2),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "       \n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = ConvAutoencoder()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,input_size = (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AutoEncoder Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for noisy_image,image in tqdm(trainloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_image = model(noisy_image)\n",
    "        loss = criterion(pred_image,image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(\"Epoch : {} Train Loss : {}\".format(i+1,train_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    dataiter = iter(trainloader)\n",
    "    sample = dataiter.next()\n",
    "    \n",
    "    noisy_image,image = sample\n",
    "    \n",
    "    index = 0 \n",
    "    \n",
    "    pred_image = model(noisy_image[index].unsqueeze(0))\n",
    "    print(pred_image.squeeze(0).shape)\n",
    "    show_image(noisy_image[index],image[index],pred_image.squeeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
